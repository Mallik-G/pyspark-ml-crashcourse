{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count Example\n",
    "\n",
    "Spark (as Hadoop) is a massively parallel system for counting words. Although DataFrames are not the perfect data structure for implementing a word count, it is still possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load text \"Alice in wonderland\"\n",
    "text = spark.read.text(\"s3://dimajix-training/data/alice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspect Schema of \"text\" DataFrame\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print first 10 entries of \"text\" DataFrame\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting words\n",
    "Now we a DataFrame with a single column. Each entry contains a line of the original text file. We need to extract the individual words in three steps:\n",
    "1. Split each line into words using the `split` function. This will result in a DataFrame with a single column, which contains a list of words\n",
    "2. Convert each list of words into individual records using the `explode` function\n",
    "3. Remove empty words using an appropriate `filter` expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Using the split function, split each record into a list of words\n",
    "word_lists = text.select(split(text.value, ' ').alias(\"word_list\"))\n",
    "# 2. Using the explode function, convert each list into individual records\n",
    "words = # YOUR CODE HERE\n",
    "# 3. Remove empty words\n",
    "non_empty_words = # YOUR CODE HERE\n",
    "# Show first 10 entries\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Words\n",
    "Now that we have a DataFrame containing an individual word per record, we can count word frequencies using grouping and aggregation.\n",
    "1. Group by word\n",
    "2. Count the size of each group\n",
    "3. Sort by frequency (descinding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. & 2. Group by \"word\" column and count the size of each group\n",
    "result = # YOUR CODE HERE\n",
    "\n",
    "# 3. Sort words by frequency (descending)\n",
    "sorted_result = # YOUR CODE HERE\n",
    "\n",
    "# Print first 10 entries (most frequent words)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark 2.1 (Python 3.5)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
